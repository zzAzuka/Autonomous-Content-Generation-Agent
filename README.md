# Autonomous-Content-Generation-Agent
This project implements an **agentic workflow for automated content creation**, capable of researching, outlining, drafting, reviewing, and evaluating articles â€” all orchestrated through a **multi-step LangGraph pipeline**.

The system integrates **human-in-the-loop checkpoints** for quality assurance and uses an **LLM-as-a-Judge** evaluator to assess the final output, ensuring both creativity and factual integrity.

---

## ğŸš€ Overview

Given a topic, the Content-Creation Agent performs the following sequence:

1. **Research** â€“ Gathers relevant information using external search tools.  
2. **Outline Generation** â€“ Produces a structured content outline.  
3. **Human Review (1)** â€“ Allows manual refinement of the outline.  
4. **Drafting** â€“ Expands the approved outline into a detailed article draft.  
5. **Human Review (2)** â€“ Invites feedback on the draft before finalization.  
6. **Final Article Generation** â€“ Produces the polished final article.  
7. **Evaluation** â€“ Uses an LLM-as-a-Judge to rate the articleâ€™s quality.

Each stage of the workflow is managed by a **distinct node** in the LangGraph framework, promoting modularity and clear separation of responsibilities.

---

## âš™ï¸ Tech Stack

| Component | Technology |
|------------|-------------|
| **Framework** | [LangGraph](https://langchain-ai.github.io/langgraph/) |
| **Model Provider** | [Google Gemini](https://deepmind.google/technologies/gemini/) |
| **Model Used** | Gemini 2.5 Flash |
| **Search Tool** | [Tavily Search](https://tavily.com/) |

---

## ğŸ§© Workflow Architecture

The workflow is designed as a **multi-node LangGraph pipeline**:
Topic Input â†’ Research Node â†’ Outline Node â†’ Human Review â†’ Draft Node â†’ Human Review â†’ Final Article Node â†’ LLM Evaluation


- Each **node** represents a distinct stage in the process.
- **Conditional edges** ensure that the workflow proceeds only after human validation or successful model execution.
- The design allows flexibility to **re-run specific stages** without restarting the entire workflow.

---

## ğŸ‘¥ Human-in-the-Loop Implementation

To ensure quality and human oversight, the system embeds **two checkpoints** for human intervention:

1. **Outline Review Checkpoint**  
   - After the outline is generated, the human user can review, edit, or approve it.  
   - The workflow halts here until feedback is provided.

2. **Draft Review Checkpoint**  
   - After the draft is created, itâ€™s presented for another round of human feedback.  
   - The user can suggest refinements before the final article is generated.

This two-step feedback mechanism ensures that the final content aligns with user intent, tone, and factual accuracy.

---

## âš–ï¸ LLM-as-a-Judge Evaluation

The **LLM Evaluation Module** functions as a standalone evaluation agent:

- It reads input text files containing the **research**, **outline**, and **draft** content.  
- The evaluator then passes these inputs to a **Judge LLM** (Gemini 2.5 Flash).  
- The Judge LLM scores the output on parameters such as:
  - Content coherence  
  - Factual consistency  
  - Depth of research  
  - Writing style and tone  
  - Overall quality  

This creates a self-assessment feedback loop that helps quantify the performance of each generated article.

---

